{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNbdVgvg/xgtcZUfe4iOO7d"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Ablation Study 1 - Simple Regression Model for Classification\n","\n","##### This notebook contains all the code necessary to perform our first ablation test. Here, we are seeing if using a simple logistic regression model based on the three peak fit parameters (height, width, and position) can successfully classify a pixel into material vs. substrate."],"metadata":{"id":"djBOyMW2xlVU"}},{"cell_type":"markdown","source":["# Load in Libraries, Data Files, and Ground Truth Data"],"metadata":{"id":"eZH3QmRHyPAO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ioGUPusaxFql"},"outputs":[],"source":["import hyperspy.api as hs\n","import hyperspy.signal_tools as hs_st\n","import hyperspy.axes as axes\n","import numpy as np\n","import pandas as pd\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","from hyperspy.signals import Signal1D\n","from tqdm import tqdm\n","import csv\n","import os\n","from sklearn import svm\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","from Ground_Truth_Creator import getGT\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.model_selection import train_test_split\n","\n","hs.set_log_level('INFO')"]},{"cell_type":"code","source":["# Loop through tif stacks to load in data as well as the ground truth results\n","file_list = []\n","for item in os.listdir('Time Series Oxidation Files'):\n","    if item.endswith('.tif'):\n","        file_list.append(item)\n","\n","data_list = []\n","for item in tqdm(file_list):\n","    data_list.append(getGT(item))"],"metadata":{"id":"U9UOz09IxTS9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Frame Creation and Visualization"],"metadata":{"id":"NQ-VZDVByauF"}},{"cell_type":"code","source":["height_Array = []\n","position_Array = []\n","width_Array = []\n","time_Array = []\n","classification_Array = []\n","\n","# Loop through .csv files containing curve fit parameters and organize arrays for\n","# each peak fit parameter\n","for file in tqdm(os.listdir('Peak Parameter CSV Files')):\n","    if file.endswith('Amp.csv'):\n","        with open(file, 'r') as f:\n","            reader = csv.reader(f)\n","            data = list(reader)\n","            amp_data = list(np.array(data).flatten())\n","            height_Array = height_Array + amp_data\n","            time = int(file.split('min')[0])\n","            time_Array = time_Array + list(np.ones(90000)*time)\n","\n","    elif file.endswith('Center.csv'):\n","        with open(file, 'r') as f:\n","            reader = csv.reader(f)\n","            data = list(reader)\n","            center_data = list(np.array(data).flatten())\n","            position_Array = position_Array + center_data\n","\n","    elif file.endswith('Sigma.csv'):\n","        with open(file, 'r') as f:\n","            reader = csv.reader(f)\n","            data = list(reader)\n","            sigma_data = list(np.array(data).flatten())\n","            width_Array = width_Array + sigma_data\n","\n","for item in data_list:\n","    classification_Array = classification_Array + item\n","\n","# Combine into one large array and process into dataframe\n","data_Array = np.array([height_Array, position_Array, width_Array, classification_Array, time_Array]).T"],"metadata":{"id":"6bLU6oXaxVLo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Curve_DF = pd.DataFrame(data_Array, columns=['Height', 'Center', 'Width', 'Classification', 'Time'], dtype='float')\n","Curve_DF"],"metadata":{"id":"tRH0DaiExZb0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Scale original data and use class_weight function to balance the mismatch in classes\n","scaler = StandardScaler()\n","x_train, x_test, y_train, y_test = train_test_split(scaler.fit_transform(Curve_DF[['Height', 'Center', 'Width']]), Curve_DF['Classification'], train_size=0.8)\n","LogReg = LogisticRegression(class_weight={1:2, 0:1}, solver='lbfgs')\n","LogReg.fit(x_train, y_train)\n","print(f\"Accuracy = {np.round(LogReg.score(x_test, y_test), 5)}\")\n","ConfusionMatrixDisplay.from_predictions(y_test, LogReg.predict(x_test), display_labels=LogReg.classes_, cmap='viridis')\n","plt.show()"],"metadata":{"id":"5YpeCcyjxbLq"},"execution_count":null,"outputs":[]}]}